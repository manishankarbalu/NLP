{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "def prepare_vocab(tokenized_corpus):\n",
    "    vocabulary = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Microsoft', 'acquired', 'Github.', 'And', 'who', 'knows', 'what', 'would', 'happen', 'to', 'Github.']\n",
      "['Microsoft', 'acquired', 'Github.', 'And', 'who', 'knows', 'what', 'would', 'happen', 'to']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"Microsoft acquired Github. And who knows what would happen to Github.\"]\n",
    "tokenized = tokenize_corpus(corpus)\n",
    "vocabulary = prepare_vocab(tokenized)\n",
    "tokenized=tokenized[0]\n",
    "print(tokenized)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Microsoft', 'acquired', 'Github.', 'And', 'who'),\n",
       " ('acquired', 'Github.', 'And', 'who', 'knows'),\n",
       " ('Github.', 'And', 'who', 'knows', 'what'),\n",
       " ('And', 'who', 'knows', 'what', 'would'),\n",
       " ('who', 'knows', 'what', 'would', 'happen'),\n",
       " ('knows', 'what', 'would', 'happen', 'to'),\n",
       " ('what', 'would', 'happen', 'to', 'Github.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokenized,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'And': 3, 'what': 6, 'would': 7, 'knows': 5, 'Github.': 2, 'acquired': 1, 'who': 4, 'to': 9, 'happen': 8, 'Microsoft': 0}\n"
     ]
    }
   ],
   "source": [
    "word2index={}\n",
    "for voca in vocabulary:\n",
    "    if word2index.get(voca)==None:\n",
    "        word2index[voca]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<DUMMY>', '<DUMMY>', 'Microsoft', 'acquired', 'Github.'), ('<DUMMY>', 'Microsoft', 'acquired', 'Github.', 'And'), ('Microsoft', 'acquired', 'Github.', 'And', 'who'), ('acquired', 'Github.', 'And', 'who', 'knows'), ('Github.', 'And', 'who', 'knows', 'what'), ('And', 'who', 'knows', 'what', 'would'), ('who', 'knows', 'what', 'would', 'happen'), ('knows', 'what', 'would', 'happen', 'to'), ('what', 'would', 'happen', 'to', 'Github.'), ('would', 'happen', 'to', 'Github.', '<DUMMY>'), ('happen', 'to', 'Github.', '<DUMMY>', '<DUMMY>')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<DUMMY>', '<DUMMY>', '<DUMMY>', 'Microsoft', 'acquired', 'Github.', 'And'),\n",
       " ('<DUMMY>', '<DUMMY>', 'Microsoft', 'acquired', 'Github.', 'And', 'who'),\n",
       " ('<DUMMY>', 'Microsoft', 'acquired', 'Github.', 'And', 'who', 'knows'),\n",
       " ('Microsoft', 'acquired', 'Github.', 'And', 'who', 'knows', 'what'),\n",
       " ('acquired', 'Github.', 'And', 'who', 'knows', 'what', 'would'),\n",
       " ('Github.', 'And', 'who', 'knows', 'what', 'would', 'happen'),\n",
       " ('And', 'who', 'knows', 'what', 'would', 'happen', 'to'),\n",
       " ('who', 'knows', 'what', 'would', 'happen', 'to', 'Github.'),\n",
       " ('knows', 'what', 'would', 'happen', 'to', 'Github.', '<DUMMY>'),\n",
       " ('what', 'would', 'happen', 'to', 'Github.', '<DUMMY>', '<DUMMY>'),\n",
       " ('would', 'happen', 'to', 'Github.', '<DUMMY>', '<DUMMY>', '<DUMMY>')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 2\n",
    "WINDOW_SIZE1 = 3\n",
    "windows = list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + tokenized + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1))\n",
    "windows1 = list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE1 + tokenized + ['<DUMMY>'] * WINDOW_SIZE1, WINDOW_SIZE1 * 2 + 1))\n",
    "print windows\n",
    "windows1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Microsoft', 'acquired'), ('Microsoft', 'Github.'), ('acquired', 'Microsoft'), ('acquired', 'Github.'), ('acquired', 'And'), ('Github.', 'Microsoft'), ('Github.', 'acquired'), ('Github.', 'And'), ('Github.', 'who'), ('And', 'acquired'), ('And', 'Github.'), ('And', 'who'), ('And', 'knows'), ('who', 'Github.'), ('who', 'And'), ('who', 'knows'), ('who', 'what'), ('knows', 'And'), ('knows', 'who'), ('knows', 'what'), ('knows', 'would'), ('what', 'who'), ('what', 'knows'), ('what', 'would'), ('what', 'happen'), ('would', 'knows'), ('would', 'what'), ('would', 'happen'), ('would', 'to'), ('happen', 'what'), ('happen', 'would'), ('happen', 'to'), ('happen', 'Github.'), ('to', 'would'), ('to', 'happen'), ('to', 'Github.'), ('Github.', 'happen'), ('Github.', 'to')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'new_trigrams = []\\n\\nfor window in windows1:\\n    c=2\\n    while c < (WINDOW_SIZE1 * 2 + 1) - 2:\\n        new_trigrams.append((window[c], window[c+1], window[c+2]))\\n        c += 2\\nprint new_trigrams'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "'''new_trigrams = []\n",
    "\n",
    "for window in windows1:\n",
    "    c=2\n",
    "    while c < (WINDOW_SIZE1 * 2 + 1) - 2:\n",
    "        new_trigrams.append((window[c], window[c+1], window[c+2]))\n",
    "        c += 2\n",
    "print new_trigrams'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 2\n",
      "[torch.LongTensor of size 1x1]\n",
      ", Variable containing:\n",
      " 9\n",
      "[torch.LongTensor of size 1x1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def prepare_word(word, word2index):\n",
    "    \n",
    "    return Variable(torch.LongTensor([word2index[word]]))\n",
    "X_p,y_p=[],[]\n",
    "\n",
    "for (center,context) in train_data:\n",
    "    X_p.append(prepare_word(center, word2index).view(1, -1))\n",
    "    y_p.append(prepare_word(context, word2index).view(1, -1))\n",
    "    \n",
    "train_data = list(zip(X_p,y_p))\n",
    "print  train_data[len(train_data)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-1.3266  1.6527  1.2775 -1.0167 -1.0160\n",
      " 0.1447  1.2489  1.0638 -2.0848 -0.7556\n",
      " 0.4026  0.8455 -2.1360 -0.1086 -0.6601\n",
      " 1.2523 -1.3154 -1.6642  0.9402  0.4511\n",
      " 0.8422 -0.9307  1.7267 -1.7794  0.0670\n",
      "-0.9816 -1.7519 -1.7928  0.0142 -1.3582\n",
      "-2.1999  1.2694 -0.3655  0.5985 -0.0742\n",
      "-0.6623  2.0116  0.6237 -0.5345  1.4169\n",
      " 1.2124 -0.2995 -1.9434 -0.2992 -0.5486\n",
      "-1.2226  0.9033 -1.0982  1.8611 -0.7820\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n",
      "Parameter containing:\n",
      " 0.5526  0.9490  0.9828 -0.1001 -1.1507\n",
      " 1.1599 -0.2314 -0.7671 -0.8115 -0.1359\n",
      "-0.7755  2.0212  0.6821  0.7479  1.9533\n",
      "-0.0649  0.7564 -0.8560 -0.1514  1.1544\n",
      "-1.1941 -1.6410 -0.0651  0.1232  1.0290\n",
      " 0.2994 -0.0119 -0.2766  0.3266  1.1709\n",
      " 1.0905  0.4083  0.8012  2.2652  0.5704\n",
      "-0.6686 -0.0272  0.7378 -1.0261 -0.3335\n",
      "-0.7826 -0.1250  0.4643  0.6210  0.3014\n",
      " 0.0656 -0.9330 -0.1016  0.9310 -0.0992\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "center_embed = nn.Embedding(len(word2index),5)\n",
    "context_embed = nn.Embedding(len(word2index),5)\n",
    "\n",
    "print(center_embed.weight)\n",
    "print(context_embed.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.3266  1.6527  1.2775 -1.0167 -1.0160\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  1.1599 -0.2314 -0.7671 -0.8115 -0.1359\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.4026  0.8455 -2.1360 -0.1086 -0.6601\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.0656 -0.9330 -0.1016  0.9310 -0.0992\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "center,context = train_data[0]\n",
    "center1,context1 = train_data[-1]\n",
    "center_vector = center_embed(center)\n",
    "context_vector = context_embed(context)\n",
    "center_vector1 = center_embed(center1)\n",
    "context_vector1 = context_embed(context1)\n",
    "\n",
    "print(center_vector)\n",
    "print(context_vector)\n",
    "print(center_vector1)\n",
    "print(context_vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1440\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5593\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = torch.exp(context_vector.bmm(center_vector.transpose(1,2))).squeeze(2)\n",
    "print score\n",
    "score1 = torch.exp(context_vector1.bmm(center_vector1.transpose(1,2))).squeeze(2)\n",
    "print score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
