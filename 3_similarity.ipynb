{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = ['Mani is hungry',\n",
    "              'Do not eat anything',\n",
    "              'Recommend a restaurant',\n",
    "              'Good food near restaurant',\n",
    "              'What will you eat fir dinner ?',\n",
    "              'I want to eat rice when I am hungry',\n",
    "              'If you are hungry eat rice',\n",
    "              'I prefer Wheat to rice for dinner',\n",
    "              'Wheat is a food',\n",
    "              'rice is a food',\n",
    "              'I want to watch movies',\n",
    "              'All movies are videos but all videos are not movies',\n",
    "              'Have you seen anything lately?',\n",
    "              'movies or entertainment recommendation',\n",
    "              'Show me some funny drama video',\n",
    "              'Give me some movies of plot with love',\n",
    "              'Back to the horror comedy movies',\n",
    "              'Show only funny videos highlights',\n",
    "              'Ram like funny and comedy movies',\n",
    "              'Wheat and rice are edible',\n",
    "              'I like to watch cricket highlights'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mani', 'is', 'hungry']\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenize_corpus(train_data)\n",
    "print tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'movies': 7, 'to': 5, 'rice': 5, 'I': 5, 'are': 4, 'eat': 4, 'videos': 3, 'food': 3, 'is': 3, 'Wheat': 3, 'you': 3, 'funny': 3, 'a': 3, 'hungry': 3, 'and': 2, 'want': 2, 'some': 2, 'highlights': 2, 'me': 2, 'watch': 2, 'not': 2, 'comedy': 2, 'like': 2, 'anything': 2, 'restaurant': 2, 'Show': 2, 'dinner': 2, 'Ram': 1, 'What': 1, 'lately?': 1, 'love': 1, 'recommendation': 1, 'am': 1, 'all': 1, 'Back': 1, 'video': 1, 'Recommend': 1, 'seen': 1, 'fir': 1, 'Do': 1, 'Good': 1, 'for': 1, 'entertainment': 1, 'plot': 1, 'when': 1, 'near': 1, 'only': 1, 'drama': 1, '?': 1, 'cricket': 1, 'Give': 1, 'prefer': 1, 'horror': 1, 'but': 1, 'Mani': 1, 'with': 1, 'edible': 1, 'All': 1, 'of': 1, 'will': 1, 'Have': 1, 'the': 1, 'or': 1, 'If': 1})\n",
      "[('If', 1), ('or', 1), ('the', 1), ('Have', 1), ('will', 1), ('of', 1), ('All', 1), ('edible', 1), ('with', 1), ('Mani', 1)]\n",
      "[('movies', 7), ('to', 5), ('rice', 5), ('I', 5), ('are', 4), ('eat', 4), ('videos', 3), ('food', 3), ('is', 3), ('Wheat', 3)]\n"
     ]
    }
   ],
   "source": [
    "#print flatten(tokenized)\n",
    "print word_count\n",
    "print list(reversed(word_count.most_common()))[:10]\n",
    "print list((word_count.most_common()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNT = 2\n",
    "stopwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'What', 'lately?', 'love', 'recommendation', 'am', 'all', 'Back', 'video', 'Recommend', 'seen', 'fir', 'Do', 'Good', 'for', 'entertainment', 'plot', 'when', 'near', 'only', 'drama', '?', 'cricket', 'Give', 'prefer', 'horror', 'but', 'Mani', 'with', 'edible', 'All', 'of', 'will', 'Have', 'the', 'or', 'If']\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# Add the word with min count in stopwords list\n",
    "for w, c in word_count.items():\n",
    "    if c < MIN_COUNT:\n",
    "        if w not in stopwords:\n",
    "            stopwords.append(w) \n",
    "print stopwords\n",
    "print len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'videos', 'is', 'some', 'are', 'want', 'funny', 'Wheat', 'highlights', 'to', 'you', 'comedy', 'a', 'food', 'watch', 'not', 'eat', 'me', 'rice', 'I', 'like', 'anything', 'restaurant', 'Show', 'hungry', 'movies', 'dinner']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(flatten(tokenized)) - set(stopwords))\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'videos': 2, 'is': 3, 'some': 4, 'are': 5, 'want': 6, 'funny': 7, 'Wheat': 8, 'highlights': 9, 'to': 10, 'you': 11, 'rice': 19, 'a': 13, 'food': 14, 'watch': 15, 'not': 16, 'eat': 17, 'me': 18, 'comedy': 12, 'I': 20, 'like': 21, 'anything': 22, 'restaurant': 23, 'Show': 24, 'hungry': 25, 'movies': 26, 'dinner': 27, '<unk>': 0}\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "word2index = {'<unk>' : 0}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "print word2index\n",
    "print len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "windows =  flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1)) for c in tokenized])\n",
    "\n",
    "#print windows\n",
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        # stopwords\n",
    "        if window[i] in stopwords or window[WINDOW_SIZE] in stopwords: \n",
    "            continue # min_count\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<unk>\"], seq))\n",
    "    #print idxs\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return torch.LongTensor([word2index[word]]) if word2index.get(word) is not None else torch.LongTensor([word2index[\"<unk>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      " 3\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 25\n",
      "[torch.LongTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#print train_data\n",
    "X_p = []\n",
    "y_p = []\n",
    "\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index))\n",
    "    y_p.append(prepare_word(tr[1], word2index))\n",
    "    \n",
    "train_data = list(zip(X_p, y_p))\n",
    "print train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as torchdata\n",
    "class WordPair(torchdata.Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        self.length = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = WordPair(train_data)\n",
    "train_loader = torchdata.DataLoader(dataset=train_data,\n",
    "                                           batch_size=16, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "word_count = Counter(flatten(tokenized)) # unigram distribution\n",
    "num_total_words = sum([c for w, c in word_count.items() if w not in stopwords])\n",
    "print num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'movies': 7, 'to': 5, 'rice': 5, 'I': 5, 'are': 4, 'eat': 4, 'videos': 3, 'food': 3, 'is': 3, 'Wheat': 3, 'you': 3, 'funny': 3, 'a': 3, 'hungry': 3, 'and': 2, 'want': 2, 'some': 2, 'highlights': 2, 'me': 2, 'watch': 2, 'not': 2, 'comedy': 2, 'like': 2, 'anything': 2, 'restaurant': 2, 'Show': 2, 'dinner': 2, 'Ram': 1, 'What': 1, 'lately?': 1, 'love': 1, 'recommendation': 1, 'am': 1, 'all': 1, 'Back': 1, 'video': 1, 'Recommend': 1, 'seen': 1, 'fir': 1, 'Do': 1, 'Good': 1, 'for': 1, 'entertainment': 1, 'plot': 1, 'when': 1, 'near': 1, 'only': 1, 'drama': 1, '?': 1, 'cricket': 1, 'Give': 1, 'prefer': 1, 'horror': 1, 'but': 1, 'Mani': 1, 'with': 1, 'edible': 1, 'All': 1, 'of': 1, 'will': 1, 'Have': 1, 'the': 1, 'or': 1, 'If': 1})\n",
      "[]\n",
      "(27, 0)\n"
     ]
    }
   ],
   "source": [
    "#print vocab\n",
    "unigram_table = []\n",
    "print word_count\n",
    "for vo in vocab:\n",
    "    unigram_table.extend([vo] * int(((word_count[vo]/num_total_words)**0.75)/0.001))\n",
    "print unigram_table\n",
    "print(len(vocab), len(unigram_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].tolist()[0]\n",
    "        while len(nsample) < k: # num of sampling\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim) # out embedding\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                \n",
    "        # xavier init\n",
    "        self.embedding_v.weight.data = nn.init.xavier_uniform(self.embedding_v.weight.data)\n",
    "        self.embedding_u.weight.data = nn.init.xavier_uniform(self.embedding_u.weight.data)\n",
    "        \n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        \n",
    "        neg_embeds = -self.embedding_u(negative_words) # B x K x D\n",
    "        \n",
    "        positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
    "        negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1) # BxK -> Bx1\n",
    "        \n",
    "        # loss function\n",
    "        loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def get_embedding(self, inputs):\n",
    "        embeds_v = self.embedding_v(inputs)\n",
    "        embeds_u = self.embedding_u(inputs)\n",
    "        \n",
    "        return (embeds_v+embeds_u)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 30 \n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100\n",
    "NEG = 10 # Num of Negative Sampling\n",
    "losses = []\n",
    "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i,(inputs,targets) in enumerate(train_loader):\n",
    "        \n",
    "        negs = negative_sampling(targets, unigram_table, NEG)\n",
    "        inputs = Variable(inputs) # B x 1\n",
    "        targets = Variable(targets) # B x 1\n",
    "        negs = Variable(negs) # B x K\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(inputs, targets, negs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.data.tolist()[0])\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(target,index2word,num=10):\n",
    "    target_V = model.get_embedding(Variable(prepare_word(target, word2index))).view(1,-1)\n",
    "    matrix = (model.embedding_u.weight.data + model.embedding_v.weight.data)/2\n",
    "    cosine_sim = F.cosine_similarity(target_V.data, matrix,dim=1,eps=1e-6)\n",
    "    v,i = cosine_sim.topk(num+1)\n",
    "    \n",
    "    return [[index2word[ii],vv] for ii,vv in zip(i.tolist()[1:],v.tolist()[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wheat', 0.3745597302913666],\n",
       " ['not', 0.2742573320865631],\n",
       " ['anything', 0.24209095537662506],\n",
       " ['Show', 0.24021531641483307],\n",
       " ['some', 0.20117609202861786],\n",
       " ['a', 0.19513079524040222],\n",
       " ['<unk>', 0.1612008959054947],\n",
       " ['and', 0.1329057216644287],\n",
       " ['to', 0.11647970974445343],\n",
       " ['watch', 0.11584723740816116]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"rice\",index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['comedy', 0.43324437737464905],\n",
       " ['Show', 0.3047579228878021],\n",
       " ['funny', 0.26918506622314453],\n",
       " ['highlights', 0.22458085417747498],\n",
       " ['videos', 0.20939500629901886],\n",
       " ['you', 0.20444093644618988],\n",
       " ['eat', 0.1973991096019745],\n",
       " ['want', 0.16375625133514404],\n",
       " ['food', 0.1545613557100296],\n",
       " ['like', 0.14094191789627075]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"movies\",index2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
